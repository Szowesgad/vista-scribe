usage: mlx_lm.generate [-h] [--model MODEL] [--trust-remote-code]
                       [--adapter-path ADAPTER_PATH]
                       [--extra-eos-token EXTRA_EOS_TOKEN [EXTRA_EOS_TOKEN ...]]
                       [--system-prompt SYSTEM_PROMPT] [--prompt PROMPT]
                       [--prefill-response PREFILL_RESPONSE]
                       [--max-tokens MAX_TOKENS] [--temp TEMP] [--top-p TOP_P]
                       [--min-p MIN_P] [--top-k TOP_K]
                       [--xtc-probability XTC_PROBABILITY]
                       [--xtc-threshold XTC_THRESHOLD]
                       [--min-tokens-to-keep MIN_TOKENS_TO_KEEP] [--seed SEED]
                       [--ignore-chat-template] [--use-default-chat-template]
                       [--chat-template-config CHAT_TEMPLATE_CONFIG]
                       [--verbose VERBOSE] [--max-kv-size MAX_KV_SIZE]
                       [--prompt-cache-file PROMPT_CACHE_FILE]
                       [--kv-bits KV_BITS] [--kv-group-size KV_GROUP_SIZE]
                       [--quantized-kv-start QUANTIZED_KV_START]
                       [--draft-model DRAFT_MODEL]
                       [--num-draft-tokens NUM_DRAFT_TOKENS]

LLM inference script

options:
  -h, --help            show this help message and exit
  --model MODEL         The path to the local model directory or Hugging Face
                        repo. If no model is specified, then mlx-
                        community/Llama-3.2-3B-Instruct-4bit is used.
  --trust-remote-code   Enable trusting remote code for tokenizer
  --adapter-path ADAPTER_PATH
                        Optional path for the trained adapter weights and
                        config.
  --extra-eos-token EXTRA_EOS_TOKEN [EXTRA_EOS_TOKEN ...]
                        Add tokens in the list of eos tokens that stop
                        generation.
  --system-prompt SYSTEM_PROMPT
                        System prompt to be used for the chat template
  --prompt PROMPT, -p PROMPT
                        Message to be processed by the model ('-' reads from
                        stdin)
  --prefill-response PREFILL_RESPONSE
                        Prefill response to be used for the chat template
  --max-tokens MAX_TOKENS, -m MAX_TOKENS
                        Maximum number of tokens to generate
  --temp TEMP           Sampling temperature
  --top-p TOP_P         Sampling top-p
  --min-p MIN_P         Sampling min-p
  --top-k TOP_K         Sampling top-k
  --xtc-probability XTC_PROBABILITY
                        Probability of XTC sampling to happen each next token
  --xtc-threshold XTC_THRESHOLD
                        Thresold the probs of each next token candidate to be
                        sampled by XTC
  --min-tokens-to-keep MIN_TOKENS_TO_KEEP
                        Minimum tokens to keep for min-p sampling.
  --seed SEED           PRNG seed
  --ignore-chat-template
                        Use the raw prompt without the tokenizer's chat
                        template.
  --use-default-chat-template
                        Use the default chat template
  --chat-template-config CHAT_TEMPLATE_CONFIG
                        Additional config for `apply_chat_template`. Should be
                        a dictionary of string keys to values represented as a
                        JSON decodable string.
  --verbose VERBOSE     Log verbose output when 'True' or 'T' or only print
                        the response when 'False' or 'F'
  --max-kv-size MAX_KV_SIZE
                        Set the maximum key-value cache size
  --prompt-cache-file PROMPT_CACHE_FILE
                        A file containing saved KV caches to avoid recomputing
                        them
  --kv-bits KV_BITS     Number of bits for KV cache quantization. Defaults to
                        no quantization.
  --kv-group-size KV_GROUP_SIZE
                        Group size for KV cache quantization.
  --quantized-kv-start QUANTIZED_KV_START
                        When --kv-bits is set, start quantizing the KV cache
                        from this step onwards.
  --draft-model DRAFT_MODEL
                        A model to be used for speculative decoding.
  --num-draft-tokens NUM_DRAFT_TOKENS
                        Number of tokens to draft when using speculative
                        decoding.
